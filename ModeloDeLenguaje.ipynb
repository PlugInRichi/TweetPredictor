{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b719c6a5",
   "metadata": {},
   "source": [
    "# Modelo de lenguaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0387c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import pprint as p\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6d62d",
   "metadata": {},
   "source": [
    "### Definición de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70ddd2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModeloB:\n",
    "    \n",
    "    def __init__(self, dim_in  = 2, h_layers = [3 , 5], dim_out = 2, lr = 0.02):\n",
    "        \"\"\"Inicialización de las matrices de la red neuronal y otros parámetros \"\"\"\n",
    "        #Establecida para experimentación\n",
    "        np.random.seed(505) \n",
    "        random.seed(505)\n",
    "        \n",
    "        self.dim_in  = dim_in # Dimensión de la entrada\n",
    "        self.dim_h_layers = len(h_layers) # Número de capas ocultas\n",
    "        self.dim_out = dim_out # Dimensión capa de salida\n",
    "\n",
    "        self.lr = lr #Learning Reate\n",
    "\n",
    "        self.layers = h_layers #Neuronas por capas internas\n",
    "        self.layers.insert(0,dim_in)\n",
    "        self.layers.append(dim_out)\n",
    "        \n",
    "        #Definición de funciones de activación\n",
    "        self.ident = lambda x : x\n",
    "        self.tanh = lambda x : np.tanh(x)\n",
    "        self.soft_max = lambda x : np.exp(x - np.max(x))/np.exp(x - np.max(x)).sum(0, keepdims=True) #Cero porque son vectores columna\n",
    "        \n",
    "        #Listas de matrices que almacenarán los pesos\n",
    "        self.weigths = list() \n",
    "        self.bias = list()\n",
    "        self.Hs = list()\n",
    "        \n",
    "        #Inicialización de matrices\n",
    "        for k in range(self.dim_h_layers + 1):\n",
    "            self.weigths.append(np.random.rand(self.layers[k+1],self.layers[k])/np.sqrt(self.layers[k+1]))#Pa que converja rápido\n",
    "            self.bias.append(np.ones((self.layers[k+1],1)))\n",
    "            self.Hs.append(np.zeros((self.layers[k+1],1)))\n",
    "\n",
    "    def _fordward(self, x):\n",
    "        \"\"\"Proceso hacia adelante para el cálculo de la predicción\"\"\"\n",
    "        #Obtiene el embedding\n",
    "        self.Hs[0] = self.weigths[0][:,x].reshape((100,1))\n",
    "        #Calcula el valor de la capa oculta\n",
    "        self.Hs[1] = self.tanh(np.dot(self.weigths[1],self.Hs[0]) + self.bias[1])\n",
    "        #Calcula el valor de la capa de salida (predicción)\n",
    "        self.Hs[2] = self.soft_max(np.dot(self.weigths[2],self.Hs[1]) + self.bias[2])     \n",
    "        \n",
    "    def _backward(self,y_idx,x_idx):\n",
    "        \"\"\"Proceso de en reversa para la propagación del error\"\"\"\n",
    "        #Creación del vector de salida\n",
    "        y = np.zeros((self.sizeVocaculary,1))\n",
    "        y[y_idx] = 1\n",
    "        \n",
    "        #Errores por capa\n",
    "        d_salida = self.Hs[-1] - y\n",
    "        d_hidden =(1 - self.Hs[-2]**2) * np.dot(self.weigths[-1].T,d_salida)\n",
    "        d_embedd =  np.dot(self.weigths[-2].T,d_hidden)\n",
    "\n",
    "        #Actualización de pesos\n",
    "        self.weigths[-1] -=  self.lr * np.outer(d_salida,self.Hs[-2])\n",
    "        self.weigths[-2] -=  self.lr * np.outer(d_hidden,self.Hs[-3])\n",
    "        self.weigths[-3][:,x_idx] -= self.lr * d_embedd.reshape(-1) \n",
    "\n",
    "        #Actualización de bias\n",
    "        self.bias[-1] = self.bias[-1] - self.lr * d_salida\n",
    "        self.bias[-2] = self.bias[-2] - self.lr * d_hidden\n",
    "        \n",
    "    def train(self, epochs = 5):\n",
    "        \"\"\"Proceso de entrenamiento utilizando Cross-Validation, se fija un número de épocas\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            for split in range(0,self.split):\n",
    "                train_split, validation_split =  self.getTrainValidSplits(split)#Obtiene los conjuntos\n",
    "                count = 0\n",
    "                for k in train_split:\n",
    "                    x = self.bigrams[k][0] #Obtención del índice de entrada asociado a la muestra k\n",
    "                    y = self.bigrams[k][1] #Obtención del índice de salida asociado a la muestra k\n",
    "                    self._fordward(x)\n",
    "                    self._backward(y,x)\n",
    "                    count += 1 \n",
    "                    if (count%5000 == 0):\n",
    "                        print('Epoch:', epoch ,' fold: ',split+1,' iteración: ',count,' / ',len(train_split))    \n",
    "                self.validation(validation_split)\n",
    "            self.crossValidation()\n",
    "                \n",
    "            \n",
    "    def validation(self,validation_split):\n",
    "        \"\"\"Procedimiento para la validación del modelo\"\"\"\n",
    "        sizeValid = len(validation_split)\n",
    "        prediciones = np.zeros(sizeValid) \n",
    "        #Obtine la probabilidad de ver y después de x para cada muestra\n",
    "        for k,i in zip(validation_split,range(sizeValid)):\n",
    "            x = self.bigrams[k][0]\n",
    "            y = self.bigrams[k][1]\n",
    "            self._fordward(x)\n",
    "            prediciones[i] = self.Hs[2][y]\n",
    "        #self.perplejidad.append(np.prod(1/prediciones)**(1/sizeValid))\n",
    "        self.entropia.append( -(1/sizeValid)*np.sum(np.log2(prediciones)))\n",
    "        print(\"Entropía parcial (por k-fold): \", self.entropia[-1],\"\\n\")\n",
    "        \n",
    "    def crossValidation(self):\n",
    "        \"\"\"Cálculo de la validación cruzada de una época\"\"\"\n",
    "        self.entropiaEpoch.append(sum(self.entropia)*(1/self.split))\n",
    "        self.entropia = list()\n",
    "        print(\"Entropía cruzada (por época): \", self.entropia[-1],\"\\n\")\n",
    "        \n",
    "    def predic(self, x):\n",
    "        \"\"\"Retorna el vector de probabilidades para la palabra representada por el índice x\"\"\"\n",
    "        self._fordward(x)\n",
    "        return self.Hs[2]\n",
    "        \n",
    "    def loadData(self, bigrams, sizeVocaculary):\n",
    "        \"\"\"Carga el conjunto de entrenamiento\"\"\"\n",
    "        self.bigrams = bigrams\n",
    "        self.sizeData = len(bigrams)\n",
    "        self.sizeVocaculary = sizeVocaculary\n",
    "        self.buildTrainValidationSplits(bigrams, 5)\n",
    "        \n",
    "    def buildTrainValidationSplits(self, bigrams, numsplit = 5):\n",
    "        \"\"\"Realiza una mezcla de los datos de entrenamiento, inicializa variables para validación\"\"\"\n",
    "        self.indexes = [i for i in range (len (bigrams))]\n",
    "        random.shuffle(self.indexes)#Mezcla los índices de cada muestra  \n",
    "        self.split = numsplit\n",
    "        self.sizeSplit = len(bigrams)//numsplit\n",
    "        \n",
    "        #self.perplejidad = list()\n",
    "        self.entropia = list()\n",
    "        self.entropiaEpoch = list()\n",
    "        \n",
    "    def getTrainValidSplits(self, k_fold):\n",
    "        \"\"\"Retorna el los conjuntos de entrenamiento y validación\n",
    "           Indicados por el parámetro k_fold\"\"\"\n",
    "        ini = (k_fold)*self.sizeSplit\n",
    "        fin = (k_fold + 1)*self.sizeSplit\n",
    "        \n",
    "        if k_fold > 0 and k_fold<self.split-1:\n",
    "            train_split = self.indexes[0:ini] + self.indexes[fin:]\n",
    "            validation_split = self.indexes[ini:fin]\n",
    "        elif k_fold == 0 :\n",
    "            train_split = self.indexes[fin:]\n",
    "            validation_split = self.indexes[0:fin]\n",
    "        else:\n",
    "            train_split = self.indexes[0:ini]\n",
    "            validation_split = self.indexes[ini:]\n",
    "        return train_split, validation_split\n",
    "            \n",
    "    def loadModel(self,file='model.pk'):\n",
    "        \"\"\"Carga de los pesos almacenados\"\"\"\n",
    "        with open(file,'rb') as model_file:\n",
    "            self.weigths = pickle.load(model_file)\n",
    "            self.bias = pickle.load(model_file)\n",
    "            #self.entropiaEpoch = pickle.load(model_file)\n",
    "        print('¡Modelo cargado correctamente!')\n",
    "    \n",
    "    def export(self,file='model_2.pk'):\n",
    "        \"\"\"Almacena los pesos de la red\"\"\"\n",
    "        with open(file,'wb') as model_file:\n",
    "            pickle.dump(self.weigths, model_file)\n",
    "            pickle.dump(self.bias, model_file)\n",
    "            pickle.dump(self.entropiaEpoch, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d4b130",
   "metadata": {},
   "source": [
    "### Carga de los bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cacf048",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Corpus.pk','rb') as corpus_file:\n",
    "    dic_words = pickle.load(corpus_file)\n",
    "    dic_index = pickle.load(corpus_file)\n",
    "    vocabulario = pickle.load(corpus_file)\n",
    "    bigrams = pickle.load(corpus_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2280b25f",
   "metadata": {},
   "source": [
    "### Ejecución de la red\n",
    "La red se entrenó en varia épocas cargando y almacenando puntos de guardado, el learning rate inicial fue de 0.1 para 1 época, posteriormente se realizaron 10 épocas con un lr = 0.05, 8 épocas con lr= 0.002 y finalmente 3 con lr = 0.005 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48624ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Modelo cargado correctamente!\n"
     ]
    }
   ],
   "source": [
    "dim = len(vocabulario)\n",
    "model = ModeloB(dim, [100 , 300], dim, 0.005) #Más alto de 0.1 hace que no converga\n",
    "model.loadData(bigrams, dim)\n",
    "model.loadModel('model_18.pk')\n",
    "#model.train(3)\n",
    "#model.export('model_21.pk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b38fa5b",
   "metadata": {},
   "source": [
    "### Las 10 probabilidades más altas por cada palabras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1388d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5539bfeee6e245e69c3a1b0d6aafe71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='word', options=('1', '10', '100', '11', '12', '13', '16', '2', '20…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def navegacion(word = sorted(list(dic_words.keys()))):\n",
    "    palabra = dic_words[word]\n",
    "    prediccion = model.predic(palabra)\n",
    "    lis = [(dic_index[idx],prob) for idx,prob in zip (range(len(prediccion.T[0])),prediccion.T[0])]\n",
    "    p.pprint(sorted(lis, key = lambda x: x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca30f3c",
   "metadata": {},
   "source": [
    "# Evaluación  del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ae37bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.loadData(bigrams, dim)\n",
    "model.loadModel('model_18.pk')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
