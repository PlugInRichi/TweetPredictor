{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae2b06e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, collections\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "#from wordcloud import WordCloud\n",
    "#from nltk import SnowballStemmer\n",
    "#from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pprint as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ea72e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData (file, symbols=''):\n",
    "    tweets = pd.read_csv(file, header=0, names=['ID', 'tweet'], sep='\\t')\n",
    "    #tweets = pd.read_csv(r'train.tsv', sep='\\t')\n",
    "    data = tweets.get('tweet').tolist()\n",
    "    clean_data = list()\n",
    "    for tweet in data:\n",
    "        url = re.sub(r'http\\S+','', tweet)\n",
    "        simbolos = re.sub(symbols, '', url)\n",
    "        usuarios = re.sub(r'@\\w+', '_user_', simbolos)\n",
    "        hashtags = re.sub(r'#\\w+', '_hashtag_', usuarios)\n",
    "        clean_data.append(hashtags.lower())\n",
    "    return data, clean_data\n",
    "\n",
    "def markData (data):\n",
    "    frases_pattern = re.compile(\n",
    "        \"(\\w+)( )?\"\n",
    "        \"(\\.|,|;|\\?|!)+\"\n",
    "        \"( )?(\\w+)\"\n",
    "    )\n",
    "    mark_data = list()\n",
    "    for tweet in data:\n",
    "        tweet = re.sub(frases_pattern, r'\\1 _EOS_  _BOS_ \\5', tweet, flags=0) #Lo deja para separa ideas\n",
    "        tweet = ' _BOS_ ' + tweet + ' _EOS_ '\n",
    "        tweet = re.sub('(\\.|,|;|\\?|!)+', r'', tweet, flags=0)\n",
    "        mark_data.append(tweet)\n",
    "    return mark_data\n",
    "\n",
    "def createTokens(mark_data):\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    tokens = tokenizer.tokenize(mark_data)\n",
    "    frequencies = collections.Counter(tokens)\n",
    "    unique_tokens = [item[0] for item in filter(lambda x: x[1] == 1, frequencies.items())]\n",
    "    return tokens, unique_tokens\n",
    "\n",
    "\n",
    "def createVocabulary (tokens, unique_tokens):\n",
    "    vocabulario = list(set(tokens)-set(unique_tokens))\n",
    "    size = len(vocabulario)\n",
    "    dic_index = dict(zip(range(size),vocabulario))  #Índice -> Palabra\n",
    "    dic_index[size] = '_OOV_' \n",
    "    dic_words = dict(zip(vocabulario, range(size))) #Palabra -> Índice\n",
    "    dic_words['_OOV_'] = size\n",
    "    return vocabulario, dic_index, dic_words\n",
    "\n",
    "\n",
    "    \n",
    "def genBigrams(dic_words,tokens,vocabulario):\n",
    "    separador = (dic_words['_EOS_'],dic_words['_BOS_'])\n",
    "    ovv = dic_words['_OOV_']\n",
    "    tokens_idx = [dic_words[token] if token in vocabulario else ovv for token in tokens]\n",
    "    #print(tokens_idx)\n",
    "    bigrams = list()\n",
    "    for i in range(len(tokens_idx) - 1):\n",
    "        if tokens_idx[i] != ovv:\n",
    "            bigrams.append((tokens_idx[i], tokens_idx[i+1]))\n",
    "    return bigrams\n",
    "    \n",
    "    \n",
    "\n",
    "def dropUniqueTokens(tokens,dic_words):\n",
    "    frequencies = collections.Counter(tokens)\n",
    "    unique_tokens = [item[0] for item in filter(lambda x: x[1] == 1, frequencies.items())]\n",
    "    delete = [(dic_words['_EOS_'],dic_words['_BOS_'])]\n",
    "    for unique in unique_tokens:\n",
    "        index = tokens.index(unique)\n",
    "        delete.append((dic_words[tokens[index]],dic_words[tokens[index+1]]))\n",
    "        delete.append((dic_words[tokens[index-1]],dic_words[tokens[index]]))\n",
    "    return delete, unique_tokens\n",
    "\n",
    "#Sub dividir la probabilidad de OVV en cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6c2e65fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras únicas: 3416 Número de tokens 31546 Tamaño no únicas 1728\n",
      "Bigramas para entrenamiento:  28129\n",
      "1728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3094"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puntuacion = r'[\\}\\{¡\"$%&\\'()¿:=\\+[\\]*-]'\n",
    "\n",
    "data, clean_data = cleanData(r'train_MX.tsv', puntuacion)\n",
    "mark_data = markData(clean_data)\n",
    "tokens, unique_tokens = createTokens(' '.join(mark_data))\n",
    "vocabulario, dic_index, dic_words = createVocabulary(tokens, unique_tokens)\n",
    "bigrams = genBigrams(dic_words,tokens,vocabulario)\n",
    "\n",
    "print('Palabras únicas:',len(unique_tokens),'Número de tokens',len(tokens),'Tamaño no únicas',len(vocabulario))\n",
    "print('Bigramas para entrenamiento: ',len(bigrams))\n",
    "print(dic_words['_OOV_'])\n",
    "len([x for x in bigrams if x[1] == dic_words['_OOV_']]) #Tamaño de token -> OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ded8a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Corpus.pk', 'wb') as corpus_file:\n",
    "    pickle.dump(dic_words, corpus_file)\n",
    "    pickle.dump(dic_index, corpus_file)\n",
    "    pickle.dump(vocabulario, corpus_file)\n",
    "    pickle.dump(bigrams, corpus_file)\n",
    "    pickle.dump(tokens, corpus_file)\n",
    "    pickle.dump(unique_tokens, corpus_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d836bf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n",
      "_B_ sadness \n",
      "\n",
      "2 3\n",
      "sadness _E_ \n",
      "\n",
      "0 1\n",
      "_B_ joy \n",
      "\n",
      "1 3\n",
      "joy _E_ \n",
      "\n",
      "0 1\n",
      "_B_ joy \n",
      "\n",
      "1 3\n",
      "joy _E_ \n",
      "\n",
      "0 5\n",
      "_B_ others \n",
      "\n",
      "5 3\n",
      "others _E_ \n",
      "\n",
      "0 2\n",
      "_B_ sadness \n",
      "\n",
      "2 3\n",
      "sadness _E_ \n",
      "\n",
      "0 1\n",
      "_B_ joy \n",
      "\n",
      "1 3\n",
      "joy _E_ \n",
      "\n",
      "0 1\n",
      "_B_ joy \n",
      "\n",
      "1 3\n",
      "joy _E_ \n",
      "\n",
      "0 5\n",
      "_B_ others \n",
      "\n",
      "5 3\n",
      "others _E_ \n",
      "\n",
      "0 2\n",
      "_B_ sadness \n",
      "\n",
      "2 3\n",
      "sadness _E_ \n",
      "\n",
      "0 5\n",
      "_B_ others \n",
      "\n",
      "5 3\n",
      "others _E_ \n",
      "\n",
      "0 5\n",
      "_B_ others \n",
      "\n",
      "5 3\n",
      "others _E_ \n",
      "\n",
      "0 5\n",
      "_B_ others \n",
      "\n",
      "5 3\n",
      "others _E_ \n",
      "\n",
      "0 2\n",
      "_B_ sadness \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for a,b in bigrams[:25]:\n",
    "    print(a,b)\n",
    "    print(dic_index[a],dic_index[b],\"\\n\")\n",
    "phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da8bd39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto Original:\n",
      " #BullyingAJos \"Mi maestra me dio un beso en la salida, por que hice los palitos derechitos...\" Pa!rodia! @JosDice\n",
      "Texto Limpio:\n",
      " _hashtag_ mi maestra me dio un beso en la salida, por que hice los palitos derechitos... pa!rodia! _user_\n",
      "Texto Marcado:\n",
      "  _BOS_ _hashtag_ mi maestra me dio un beso en la salida _EOS_  _BOS_ por que hice los palitos derechitos _EOS_  _BOS_ parodia _EOS_  _BOS_ _user_ _EOS_  \n",
      "\n",
      "Texto Original:\n",
      " ¡Ay como sufren!!! Desde tempranito empezaron a cortarse las venas... Mejor vayan a las posadas, aunque sea pescan a la señora que reza\n",
      "Texto Limpio:\n",
      " ay como sufren!!! desde tempranito empezaron a cortarse las venas... mejor vayan a las posadas, aunque sea pescan a la señora que reza\n",
      "Texto Marcado:\n",
      "  _BOS_ ay como sufren _EOS_  _BOS_ desde tempranito empezaron a cortarse las venas _EOS_  _BOS_ mejor vayan a las posadas _EOS_  _BOS_ aunque sea pescan a la señora que reza _EOS_  \n",
      "\n",
      "Texto Original:\n",
      " Si definieran su primer día del 2017 con una palabra cuál sería? Respondan citando #Este tuit\n",
      "Texto Limpio:\n",
      " si definieran su primer día del 2017 con una palabra cuál sería? respondan citando _hashtag_ tuit\n",
      "Texto Marcado:\n",
      "  _BOS_ si definieran su primer día del 2017 con una palabra cuál sería _EOS_  _BOS_ respondan citando _hashtag_ tuit _EOS_  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Texto Original:\\n\",data[66])\n",
    "print(\"Texto Limpio:\\n\",clean_data[66])\n",
    "print(\"Texto Marcado:\\n\",mark_data[66],\"\\n\")\n",
    "\n",
    "print(\"Texto Original:\\n\",data[71])\n",
    "print(\"Texto Limpio:\\n\",clean_data[71])\n",
    "print(\"Texto Marcado:\\n\",mark_data[71],\"\\n\")\n",
    "\n",
    "print(\"Texto Original:\\n\",data[114])\n",
    "print(\"Texto Limpio:\\n\",clean_data[114])\n",
    "print(\"Texto Marcado:\\n\",mark_data[114],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6bbc09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
