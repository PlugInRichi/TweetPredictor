{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae2b06e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, collections\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "#from wordcloud import WordCloud\n",
    "#from nltk import SnowballStemmer\n",
    "#from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "#import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea72e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData (file, symbols=''):\n",
    "    tweets = pd.read_csv(file, header=0, names=['ID', 'tweet'], sep='\\t')\n",
    "    data = tweets.get('tweet').tolist()\n",
    "    clean_data = list()\n",
    "    for tweet in data:\n",
    "        url = re.sub(r'http\\S+','', tweet)\n",
    "        simbolos = re.sub(symbols, '', url)\n",
    "        usuarios = re.sub(r'@\\w+', '_user_', simbolos)\n",
    "        hashtags = re.sub(r'#\\w+', '_hashtag_', usuarios)\n",
    "        clean_data.append(hashtags.lower())\n",
    "    return data, clean_data\n",
    "\n",
    "def markData (data):\n",
    "    frases_pattern = re.compile(\n",
    "        \"(\\w+)( )?\"\n",
    "        \"[\\.|,|;|\\?|!]+\"\n",
    "        \"( )?(\\w+)\"\n",
    "    )\n",
    "    mark_data = list()\n",
    "    for tweet in data:\n",
    "        tweet = ' _B_ ' + tweet + ' _E_ '\n",
    "        tweet = re.sub(frases_pattern, r'\\1 _E_  _B_ \\4', tweet, flags=0) #Lo deja para separa ideas\n",
    "        mark_data.append(tweet)\n",
    "    return mark_data\n",
    "\n",
    "\n",
    "def createVocabulary (data, load=False, file='vocabulary.pk'):\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    tokens = tokenizer.tokenize(data_mark)\n",
    "    if(load):\n",
    "        with open('vocabulary.pk','rb') as vocabulary_file:\n",
    "            dic_words = pickle.load(vocabulary_file)\n",
    "            dic_index = pickle.load(vocabulary_file)\n",
    "            vocabulario = pickle.load(vocabulary_file)\n",
    "    else\n",
    "        dic_index = dict(zip(range(len(vocabulario)),vocabulario))  #Índice -> Palabra\n",
    "        dic_words = dict(zip(vocabulario, range(len(vocabulario)))) #Palabra -> Índice\n",
    "        vocabulario = list(set(tokens))\n",
    "        with open('vocabulary.pk','wb') as vocabulary_file:\n",
    "            pickle.dump(dic_words, vocabulary_file)\n",
    "            pickle.dump(dic_index, vocabulary_file)\n",
    "            pickle.dump(vocabulario, vocabulary_file)\n",
    "    return vocabulario, dic_index, dic_words\n",
    "\n",
    "\n",
    "    \n",
    "def genBigrams(dic_words,tokens,delete):\n",
    "    corpus_index = [dic_words[token] for token in tokens]\n",
    "    bigrams = list()\n",
    "    for i in range(len(corpus_index)-1):\n",
    "        bigram = (corpus_index[i], corpus_index[i+1])\n",
    "        if bigram not in delete:\n",
    "            bigrams.append(bigram)\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "\n",
    "def trainData():\n",
    "    genBigrams()\n",
    "    #Si el bigrama comienza con una palabra atípica, agrega a la lista\n",
    "    phrase = (dic_words['_E_'],dic_words['_B_']) #evita incluir _E_ _B_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6c2e65fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "puntuacion = r'[\\}\\{¡\"$%&\\'()¿:=\\+[\\]*-]'\n",
    "\n",
    "data, clean_data = cleanData(r'train_MX.tsv', puntuacion)\n",
    "mark_data = markData(data)\n",
    "\n",
    "\n",
    "vocabulario = list(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "da8bd39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto Original:\n",
      " #BullyingAJos \"Mi maestra me dio un beso en la salida, por que hice los palitos derechitos...\" Pa!rodia! @JosDice\n",
      "Texto Limpio:\n",
      " _hashtag_ mi maestra me dio un beso en la salida, por que hice los palitos derechitos... pa!rodia! _user_\n",
      "Texto Marcado:\n",
      "  _B_ #BullyingAJos \"Mi maestra me dio un beso en la salida _E_  _B_ por que hice los palitos derechitos...\" Pa _E_  _B_ rodia! @JosDice _E_  \n",
      "\n",
      "Texto Original:\n",
      " ¡Ay como sufren!!! Desde tempranito empezaron a cortarse las venas... Mejor vayan a las posadas, aunque sea pescan a la señora que reza\n",
      "Texto Limpio:\n",
      " ay como sufren!!! desde tempranito empezaron a cortarse las venas... mejor vayan a las posadas, aunque sea pescan a la señora que reza\n",
      "Texto Marcado:\n",
      "  _B_ ¡Ay como sufren _E_  _B_ Desde tempranito empezaron a cortarse las venas _E_  _B_ Mejor vayan a las posadas _E_  _B_ aunque sea pescan a la señora que reza _E_  \n",
      "\n",
      "Texto Marcado:\n",
      "  _B_ Si definieran su primer día del 2017 con una palabra cuál sería _E_  _B_ Respondan citando #Este tuit _E_  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Texto Original:\\n\",data[66])\n",
    "print(\"Texto Limpio:\\n\",clean_data[66])\n",
    "print(\"Texto Marcado:\\n\",mark_data[66],\"\\n\")\n",
    "\n",
    "print(\"Texto Original:\\n\",data[71])\n",
    "print(\"Texto Limpio:\\n\",clean_data[71])\n",
    "print(\"Texto Marcado:\\n\",mark_data[71],\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Texto Marcado:\\n\",mark_data[114],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6bbc09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
