{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae2b06e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, collections\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "#from wordcloud import WordCloud\n",
    "#from nltk import SnowballStemmer\n",
    "#from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea72e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData (file, symbols=''):\n",
    "    tweets = pd.read_csv(file, header=0, names=['ID', 'tweet'], sep='\\t')\n",
    "    data = tweets.get('tweet').tolist()\n",
    "    clean_data = list()\n",
    "    for tweet in data:\n",
    "        url = re.sub(r'http\\S+','', tweet)\n",
    "        simbolos = re.sub(symbols, '', url)\n",
    "        usuarios = re.sub(r'@\\w+', '_user_', simbolos)\n",
    "        hashtags = re.sub(r'#\\w+', '_hashtag_', usuarios)\n",
    "        clean_data.append(hashtags.lower())\n",
    "    return data, clean_data\n",
    "\n",
    "def markData (data):\n",
    "    frases_pattern = re.compile(\n",
    "        \"(\\w+)( )?\"\n",
    "        \"[\\.|,|;|\\?|!]+\"\n",
    "        \"( )?(\\w+)\"\n",
    "    )\n",
    "    mark_data = list()\n",
    "    for tweet in data:\n",
    "        tweet = ' _B_ ' + tweet + ' _E_ '\n",
    "        tweet = re.sub(frases_pattern, r'\\1 _E_  _B_ \\4', tweet, flags=0) #Lo deja para separa ideas\n",
    "        mark_data.append(tweet)\n",
    "    return mark_data\n",
    "\n",
    "\n",
    "def createVocabulary (data_mark, load=False, file='vocabulary_tweets_MX.pk'):\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    tokens = tokenizer.tokenize(data_mark)\n",
    "    if(load):\n",
    "        with open(file,'rb') as vocabulary_file:\n",
    "            dic_words = pickle.load(vocabulary_file)\n",
    "            dic_index = pickle.load(vocabulary_file)\n",
    "            vocabulario = pickle.load(vocabulary_file)\n",
    "    else:\n",
    "        vocabulario = list(set(tokens))\n",
    "        dic_index = dict(zip(range(len(vocabulario)),vocabulario))  #Índice -> Palabra\n",
    "        dic_words = dict(zip(vocabulario, range(len(vocabulario)))) #Palabra -> Índice\n",
    "        with open(file, 'wb') as vocabulary_file:\n",
    "            pickle.dump(dic_words, vocabulary_file)\n",
    "            pickle.dump(dic_index, vocabulary_file)\n",
    "            pickle.dump(vocabulario, vocabulary_file)\n",
    "    return vocabulario,tokens, dic_index, dic_words\n",
    "\n",
    "\n",
    "    \n",
    "def genBigrams(dic_words,tokens,delete):\n",
    "    corpus_index = [dic_words[token] for token in tokens]\n",
    "    bigrams = list()\n",
    "    for i in range(len(corpus_index)-1):\n",
    "        bigram = (corpus_index[i], corpus_index[i+1])\n",
    "        if bigram not in delete:\n",
    "            bigrams.append(bigram)\n",
    "    return bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c2e65fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "puntuacion = r'[\\}\\{¡\"$%&\\'()¿:=\\+[\\]*-]'\n",
    "\n",
    "data, clean_data = cleanData(r'train_MX.tsv', puntuacion)\n",
    "mark_data = markData(clean_data)\n",
    "\n",
    "vocabulario,tokens, dic_index, dic_words = createVocabulary (''.join(mark_data))\n",
    "frequencies = collections.Counter(tokens)\n",
    "\n",
    "phrase = (dic_words['_E_'],dic_words['_B_'])\n",
    "bigrams = genBigrams(dic_words,tokens,[phrase])\n",
    "\n",
    "with open('Corpus.pk', 'wb') as corpus_file:\n",
    "    pickle.dump(dic_words, corpus_file)\n",
    "    pickle.dump(dic_index, corpus_file)\n",
    "    pickle.dump(vocabulario, corpus_file)\n",
    "    pickle.dump(bigrams,corpus_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3b99058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3415"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [item[0] for item in filter(lambda x: x[1] == 1, frequencies.items())]\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b282354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3318 1138\n",
      "_B_ por \n",
      "\n",
      "1138 1911\n",
      "por fin \n",
      "\n",
      "1911 2518\n",
      "fin terminando \n",
      "\n",
      "2518 3478\n",
      "terminando mis \n",
      "\n",
      "3478 2506\n",
      "mis obras \n",
      "\n",
      "2506 72\n",
      "obras y \n",
      "\n",
      "72 4088\n",
      "y ebbos \n",
      "\n",
      "4088 4263\n",
      "ebbos vamos \n",
      "\n",
      "4263 1138\n",
      "vamos por \n",
      "\n",
      "1138 3583\n",
      "por la \n",
      "\n",
      "3583 2647\n",
      "la última \n",
      "\n",
      "2647 72\n",
      "última y \n",
      "\n",
      "72 293\n",
      "y a \n",
      "\n",
      "293 4442\n",
      "a terminar \n",
      "\n",
      "4442 2171\n",
      "terminar todo \n",
      "\n",
      "2171 2926\n",
      "todo gracias \n",
      "\n",
      "2926 293\n",
      "gracias a \n",
      "\n",
      "293 3478\n",
      "a mis \n",
      "\n",
      "3478 690\n",
      "mis santos \n",
      "\n",
      "690 4965\n",
      "santos _E_ \n",
      "\n",
      "3318 1546\n",
      "_B_ egguns \n",
      "\n",
      "1546 72\n",
      "egguns y \n",
      "\n",
      "72 293\n",
      "y a \n",
      "\n",
      "293 3194\n",
      "a mi \n",
      "\n",
      "3194 3452\n",
      "mi hermano \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4965, 3318)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for a,b in bigrams[:25]:\n",
    "    print(a,b)\n",
    "    print(dic_index[a],dic_index[b],\"\\n\")\n",
    "phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da8bd39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto Original:\n",
      " #BullyingAJos \"Mi maestra me dio un beso en la salida, por que hice los palitos derechitos...\" Pa!rodia! @JosDice\n",
      "Texto Limpio:\n",
      " _hashtag_ mi maestra me dio un beso en la salida, por que hice los palitos derechitos... pa!rodia! _user_\n",
      "Texto Marcado:\n",
      "  _B_ #BullyingAJos \"Mi maestra me dio un beso en la salida _E_  _B_ por que hice los palitos derechitos...\" Pa _E_  _B_ rodia! @JosDice _E_  \n",
      "\n",
      "Texto Original:\n",
      " ¡Ay como sufren!!! Desde tempranito empezaron a cortarse las venas... Mejor vayan a las posadas, aunque sea pescan a la señora que reza\n",
      "Texto Limpio:\n",
      " ay como sufren!!! desde tempranito empezaron a cortarse las venas... mejor vayan a las posadas, aunque sea pescan a la señora que reza\n",
      "Texto Marcado:\n",
      "  _B_ ¡Ay como sufren _E_  _B_ Desde tempranito empezaron a cortarse las venas _E_  _B_ Mejor vayan a las posadas _E_  _B_ aunque sea pescan a la señora que reza _E_  \n",
      "\n",
      "Texto Original:\n",
      " Si definieran su primer día del 2017 con una palabra cuál sería? Respondan citando #Este tuit\n",
      "Texto Limpio:\n",
      " si definieran su primer día del 2017 con una palabra cuál sería? respondan citando _hashtag_ tuit\n",
      "Texto Marcado:\n",
      "  _B_ Si definieran su primer día del 2017 con una palabra cuál sería _E_  _B_ Respondan citando #Este tuit _E_  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Texto Original:\\n\",data[66])\n",
    "print(\"Texto Limpio:\\n\",clean_data[66])\n",
    "print(\"Texto Marcado:\\n\",mark_data[66],\"\\n\")\n",
    "\n",
    "print(\"Texto Original:\\n\",data[71])\n",
    "print(\"Texto Limpio:\\n\",clean_data[71])\n",
    "print(\"Texto Marcado:\\n\",mark_data[71],\"\\n\")\n",
    "\n",
    "print(\"Texto Original:\\n\",data[114])\n",
    "print(\"Texto Limpio:\\n\",clean_data[114])\n",
    "print(\"Texto Marcado:\\n\",mark_data[114],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6bbc09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
